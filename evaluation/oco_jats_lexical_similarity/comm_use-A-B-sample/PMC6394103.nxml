<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.0 20120330//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article"><?properties open_access?><front><journal-meta><journal-id journal-id-type="nlm-ta">Burns Trauma</journal-id><journal-id journal-id-type="iso-abbrev">Burns Trauma</journal-id><journal-title-group><journal-title>Burns &#x00026; Trauma</journal-title></journal-title-group><issn pub-type="ppub">2321-3868</issn><issn pub-type="epub">2321-3876</issn><publisher><publisher-name>BioMed Central</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="pmid">30859107</article-id><article-id pub-id-type="pmc">6394103</article-id><article-id pub-id-type="publisher-id">137</article-id><article-id pub-id-type="doi">10.1186/s41038-018-0137-9</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research Article</subject></subj-group></article-categories><title-group><article-title>Burn image segmentation based on Mask Regions with Convolutional Neural Network deep learning framework: more accurate and more convenient</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Jiao</surname><given-names>Chong</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Su</surname><given-names>Kehua</given-names></name><address><email>skh@whu.edu.cn</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Xie</surname><given-names>Weiguo</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Ye</surname><given-names>Ziqing</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2331 6153</institution-id><institution-id institution-id-type="GRID">grid.49470.3e</institution-id><institution>School of Computer Science, </institution><institution>Wuhan University, </institution></institution-wrap>Wuhan, 430072 China </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0001 2331 6153</institution-id><institution-id institution-id-type="GRID">grid.49470.3e</institution-id><institution>Institute of Burns, </institution><institution>Wuhan Hospital No. 3 and Tongren Hospital of Wuhan University, </institution></institution-wrap>Wuhan, 430060 China </aff></contrib-group><pub-date pub-type="epub"><day>28</day><month>2</month><year>2019</year></pub-date><pub-date pub-type="pmc-release"><day>28</day><month>2</month><year>2019</year></pub-date><pub-date pub-type="collection"><year>2019</year></pub-date><volume>7</volume><elocation-id>6</elocation-id><history><date date-type="received"><day>17</day><month>7</month><year>2018</year></date><date date-type="accepted"><day>30</day><month>11</month><year>2018</year></date></history><permissions><copyright-statement>&#x000a9; The Author(s) 2019</copyright-statement><license license-type="OpenAccess"><license-p><bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (<ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated.</license-p></license></permissions><abstract id="Abs1"><sec><title>Background</title><p id="Par1">Burns are life-threatening with high morbidity and mortality. Reliable diagnosis supported by accurate burn area and depth assessment is critical to the success of the treatment decision and, in some cases, can save the patient&#x02019;s life. Current techniques such as straight-ruler method, aseptic film trimming method, and digital camera photography method are not repeatable and comparable, which lead to a great difference in the judgment of burn wounds and impede the establishment of the same evaluation criteria. Hence, in order to semi-automate the burn diagnosis process, reduce the impact of human error, and improve the accuracy of burn diagnosis, we include the deep learning technology into the diagnosis of burns.</p></sec><sec><title>Method</title><p id="Par2">This article proposes a novel method employing a state-of-the-art deep learning technique to segment the burn wounds in the images. We designed this deep learning segmentation framework based on the Mask Regions with Convolutional Neural Network (Mask R-CNN). For training our framework, we labeled 1150 pictures with the format of the Common Objects in Context (COCO) data set and trained our model on 1000 pictures. In the evaluation, we compared the different backbone networks in our framework. These backbone networks are Residual Network-101 with Atrous Convolution in Feature Pyramid Network (R101FA), Residual Network-101 with Atrous Convolution (R101A), and InceptionV2-Residual Network with Atrous Convolution (IV2RA). Finally, we used the Dice coefficient (DC) value to assess the model accuracy.</p></sec><sec><title>Result</title><p id="Par3">The R101FA backbone network gains the highest accuracy 84.51% in 150 pictures. Moreover, we chose different burn depth pictures to evaluate these three backbone networks. The R101FA backbone network gains the best segmentation effect in superficial, superficial thickness, and deep partial thickness. The R101A backbone network gains the best segmentation effect in full-thickness burn.</p></sec><sec><title>Conclusion</title><p id="Par4">This deep learning framework shows excellent segmentation in burn wound and extremely robust in different burn wound depths. Moreover, this framework just needs a suitable burn wound image when analyzing the burn wound. It is more convenient and more suitable when using in clinics compared with the traditional methods. And it also contributes more to the calculation of total body surface area (TBSA) burned.</p></sec></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Burn image</kwd><kwd>Deep learning</kwd><kwd>Mask R-CNN</kwd><kwd>Image segmentation</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#x000a9; The Author(s) 2019</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Background</title><p id="Par18">Burn injuries require immediate treatment by estimating the burn area and burn depth. Normally, this work is hard to solve by the general nurse or the doctor.</p><sec id="Sec2"><title>Current calculation methods</title><p id="Par19">As shown in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>, the current assessment of a burn wound consists of three main methods [<xref ref-type="bibr" rid="CR1">1</xref>&#x02013;<xref ref-type="bibr" rid="CR4">4</xref>]. The first method is to attach the sterile film to the wound of the patient, then draw the wound boundary with a marking pen, and finally calculate the area of the wound. This method is influenced by the subjective factors of the marking person and will bring about errors in some depths. The second method is to use the digital camera to calculate the wound area based on the principle of camera imaging. Then, this digital camera uses NIH ImageJ software to calculate the wound area. However, this software cannot automatically identify the edge of the wound, which makes it necessary to label the wound edge before calculating the wound area. The third method is to use the BurnCalc system [<xref ref-type="bibr" rid="CR4">4</xref>]. The doctor uses this system to build 3D models of patients through specialized 3D scanning equipment, then draws the wound on the 3D model using a special drawing software, and finally calculates the wound area. This method needs to draw the artificial wound, which causes the final area calculation result to be highly inaccurate. In 2018, Cheah et al. designed a 3D application to calculate the burn area [<xref ref-type="bibr" rid="CR5">5</xref>]. This application builds the 3D body model by the people&#x02019;s height and weight. Then, the doctors mark the burn area on the surface of the 3D model. After marking, the application can calculate the burn area automatically. However, this calculation relies on the precise burn area marked. Moreover, accurately marking the burn area is a time-consuming work which is not suitable for busy first-aid work.<fig id="Fig1"><label>Fig. 1</label><caption><p>Current mainstream diagnostic methods of burn wounds</p></caption><graphic xlink:href="41038_2018_137_Fig1_HTML" id="MO1"/></fig></p><p id="Par20">We can conclude from above that the complicated and time-consuming part in these methods is to determine the wound region from the patient&#x02019;s skin, which is where many errors occur. To solve this problem, we used state-of-the-art deep learning techniques to segment the wound region, and this method can be well applied to the calculation of the wound area.</p></sec><sec id="Sec3"><title>Object detection</title><p id="Par21">Analyzing object recognition and location in images is one of the most fundamental and challenging problems in computer vision. In the deep learning, Regions with Convolutional Neural Network (R-CNN) [<xref ref-type="bibr" rid="CR6">6</xref>] is an object detection method in imagery analysis. This system consists of three parts. The first part generates category-independent region proposals by selective search [<xref ref-type="bibr" rid="CR7">7</xref>]. The second part is a large convolutional neural network that extracts a fixed-length feature vector from each region. The last part is a set of class-specific linear support vector machines (SVM).</p><p id="Par22">In order to realize faster training and evaluating, the Fast R-CNN [<xref ref-type="bibr" rid="CR8">8</xref>] obtains feature vectors from the shared feature map. To avoid region proposals distortion, it adds a region of interest (RoI) pooling layer [<xref ref-type="bibr" rid="CR8">8</xref>] on the basis of the R-CNN. Moreover, the author used the multi-task loss in the classification and regression to accelerate the system. In 2016, the Faster R-CNN [<xref ref-type="bibr" rid="CR9">9</xref>] applied the region proposal network (RPN) [<xref ref-type="bibr" rid="CR9">9</xref>] network to accelerate the speed of the generation of category-independent region proposals. With the powerful deep learning baseline, the object detection results can reach a high standard.</p></sec><sec id="Sec4"><title>RPN</title><p id="Par23">In the early object detection system, the generation of region proposals is exhaustive search. Then, in 2013, Uijlings et al. proposed a selective search to relieve the computational pressure [<xref ref-type="bibr" rid="CR7">7</xref>]. Ren et al. then proposed an RPN which generates high-quality region proposals from the shared feature maps [<xref ref-type="bibr" rid="CR9">9</xref>]. In generation, the RPN used a small sliding window on the shared feature map. Then, at each sliding-window location, the RPN generated multiple scale rectangle boxes on the original image, and these rectangle boxes are called anchors [<xref ref-type="bibr" rid="CR9">9</xref>]. After that, the RPN network used a small convolutional neural network to predict the object score and box regression at each anchor. Then, the RPN sorted the score of each anchor and used the greedy non-maximum suppression to obtain region proposals.</p></sec><sec id="Sec5"><title>Residual network (ResNet)</title><p id="Par24">ResNet [<xref ref-type="bibr" rid="CR10">10</xref>] was proposed in 2016 by Kaiming et al. In the convolutional neural network, the deeper the network is, the more features it can obtain. Although this feature of the convolutional neural network leads to a series of breakthroughs in image classification, it also brings a lot of new problems. For example, as the network depth deepens, the notorious problems of vanishing/exploding gradient occur. The deep residual learning network, which adds a reference on each layer to learn the residual function, can address the degradation problem properly. The main networks in the ResNet are the ResNet101 [<xref ref-type="bibr" rid="CR10">10</xref>] and ResNet50 [<xref ref-type="bibr" rid="CR10">10</xref>].</p></sec><sec id="Sec6"><title>Feature pyramid network (FPN)</title><p id="Par25">FPN [<xref ref-type="bibr" rid="CR11">11</xref>] was proposed in 2017 by the Lin et al. This network can build high-level semantic feature maps at all scales. It contains three parts which are bottom-up, top-down, and lateral connections [<xref ref-type="bibr" rid="CR11">11</xref>]. In general, ResNet is the backbone network of the FPN. The output feature map of each convolution layer is denoted as C<sub>2</sub>, C<sub>3</sub>, C<sub>4</sub>, and C<sub>5</sub> in the bottom-up part. The top-down part then upsamples the feature map by using a factor of 2, and the upsampled map is merged with the corresponding bottom-up map. Then a 3&#x000a0;&#x000d7;&#x000a0;3 convolution is appended on each merged map to generate the final feature map. The final outputs of feature maps are called P<sub>2</sub>, P<sub>3</sub>, P<sub>4</sub>, and P<sub>5</sub>. In this article, we adopt the ResNet101 and Atrous [<xref ref-type="bibr" rid="CR12">12</xref>] in FPN, and we called this backbone network as R101FA.</p></sec><sec id="Sec7"><title>Mask R-CNN</title><p id="Par26">In principle, Mask R-CNN is an intuitive extension of Faster R-CNN, yet building the mask branch properly is important for good results. Mask R-CNN is different from the Faster R-CNN in three points. The first point is that the Mask R-CNN adds a mask branch to predict the category of every pixel in the region proposals. The second point is that the use of the RoIAlign [<xref ref-type="bibr" rid="CR13">13</xref>] to achieve the pixel-pixel alignment between network inputs and outputs. The third point is the definition of the mask loss. For each RoI associated with ground-truth class k, mask loss is only defined on the <italic>k</italic>-th mask (other mask outputs do not contribute to the loss).</p></sec></sec><sec id="Sec8"><title>Method</title><p id="Par27">In this article, we used a novel method for employing the state-of-the-art deep learning framework Mask R-CNN. For a more refined result and a faster training speed, we modified the Mask R-CNN to adapt to our dataset. We changed the loss function of the class branch and adopted the Atrous Convolution [<xref ref-type="bibr" rid="CR12">12</xref>] in the backbone network. In order to make a better segmentation result, we tried several mainstream backbone networks, and the R101FA demonstrated the best segmentation results.</p><sec id="Sec9"><title>Data set</title><p id="Par28">From December 2017 to June 2018, we worked with the burn department of the Wuhan Hospital No. 3. Ethics approvals were granted by the Wuhan Hospital No. 3 and Tongren Hospital of Wuhan University. The patients used in this research have already signed the informed consent.</p><p id="Par29">In order to obtain enough data, we used our smartphone to collect images of fresh burn wounds in the hospital every day. Then, we used our own software to annotate burn images and saved the marked content in Common Objects in Context (COCO) data set format. Figure&#x000a0;<xref rid="Fig2" ref-type="fig">2</xref> shows the software which we used to annotate. In order to ensure the accuracy of this framework, we annotated the burn images carefully under the guidance of the professional doctors and avoided mistaking confusing parts such as gauze and blood stains as wounds. With the help of doctors and nurses, we finally annotated 1000 burn images for training and another 150 for evaluating.<fig id="Fig2"><label>Fig. 2</label><caption><p>Annotation tool. With the help of professional doctors, we use the corresponding annotation tool to annotate the image data. This annotation tool is made with QT (a C++ framework)</p></caption><graphic xlink:href="41038_2018_137_Fig2_HTML" id="MO2"/></fig></p></sec><sec id="Sec10"><title>Network architecture</title><p id="Par30">As shown in Fig.&#x000a0;<xref rid="Fig3" ref-type="fig">3</xref>, our framework contains three parts. The first part is the backbone network to extract the feature maps. The second part is the RPN [<xref ref-type="bibr" rid="CR9">9</xref>] network to generate the RoI [<xref ref-type="bibr" rid="CR9">9</xref>]. Finally, we process the object detection and mask prediction from each RoI. Because there is only one category (here, we do not consider the depth of burn wound), we changed the loss function of the mask branch and classification branch to fit our data set. In the process of training, we collected almost all kinds of burn wound images to train our model, totaling 1000 after filtering. At the same time, in order to realize faster training speed and less evaluating time, we tried different backbone networks in our framework. Finally, we used the R101FA as the backbone network of our framework.<fig id="Fig3"><label>Fig. 3</label><caption><p>Block diagram of the burn image segmentation framework. The framework contains three parts. The blue part is extracting the features from the image. The red part is the region proposal network to generate the regions of interests. The green part is the network heads to classify and regress</p></caption><graphic xlink:href="41038_2018_137_Fig3_HTML" id="MO3"/></fig></p><p id="Par31">In this article, our backbone network is based on the R101FA. The ResNet101 is made up of 101 layers. We use C<sub>1</sub>, C<sub>2</sub>, C<sub>3</sub>, C<sub>4</sub>, and C<sub>5</sub> to define these output feature maps. As shown in Fig.&#x000a0;<xref rid="Fig4" ref-type="fig">4</xref>, we obtain final feature maps P<sub>2</sub>, P<sub>3</sub>, P<sub>4</sub>, and P<sub>5</sub>.<fig id="Fig4"><label>Fig. 4</label><caption><p>The process of generating feature map. The left column is the bottom-up of the feature pyramid network (FPN). The right column is the top-down of the FPN. The middle column is the lateral connection of the FPN</p></caption><graphic xlink:href="41038_2018_137_Fig4_HTML" id="MO4"/></fig></p><p id="Par32">Here, we use a 1&#x000a0;&#x000d7;&#x000a0;1 convolution kernel to get the first feature map P<sub>5</sub> by undergoing the output of the C<sub>5</sub>. Then, we upsample the P<sub>5</sub> to get the P<sup>*</sup> and produce C<sup>*</sup> by the 3&#x000a0;&#x000d7;&#x000a0;3 convolution kernel undergoing the C<sub>5</sub>, and the P<sub>4</sub> map is the result of merging C<sup>*</sup> with P<sup>*</sup>. After iterating all C, we can build P<sub>2</sub>, P<sub>3</sub>, P<sub>4</sub>, and P<sub>5</sub>.</p></sec><sec id="Sec11"><title>Atrous</title><p id="Par33">In the convolutional neural network, we employ atrous convolution [<xref ref-type="bibr" rid="CR12">12</xref>] in ResNet. The traditional convolution kernel is usually composed of a dense matrix of N&#x02009;&#x000d7;&#x02009;N. The kernel of atrous convolution is no longer a dense matrix, and it is shown by Fig.&#x000a0;<xref rid="Fig5" ref-type="fig">5</xref> that different rates represent different convolution kernels. Compared with traditional convolution kernels, employing a larger value of atrous rate enlarges the model&#x02019;s field-of-view, enabling object encoding at multiple scales. This structure is suited for our burn dataset which consists of varying burn depths and burn sizes. In this research, we set the rate at 2.<fig id="Fig5"><label>Fig. 5</label><caption><p>Atrous convolution. Atrous convolution with kernel size 3&#x02009;&#x000d7;&#x02009;3 and different rates. From left to right in the figure, the atrous rates are 1, 2, and 3. Standard convolution corresponds to atrous convolution with rate&#x02009;=&#x02009;1. In this article, we set the rate as 2</p></caption><graphic xlink:href="41038_2018_137_Fig5_HTML" id="MO5"/></fig></p></sec><sec id="Sec12"><title>RPN in FPN</title><p id="Par34">We adopt RPN in FPN to propose the candidate of region proposals. The detail is different from the original RPN network. The original RPN network just adopts one feature map, but in our network, we build several feature maps. In order to handle the images easier, we resized the images to 1024&#x000a0;&#x000d7;&#x000a0;1024 and filled the image with zero to prevent distortion. In order to contain all possible rectangular boxes, we defined five scales 32&#x02009;&#x000d7;&#x02009;32, 64&#x02009;&#x000d7;&#x02009;64, 128&#x02009;&#x000d7;&#x02009;128, 256&#x02009;&#x000d7;&#x02009;256, and 512&#x02009;&#x000d7;&#x02009;512. Every scale has three aspect ratios 0.5, 1, and 2. It was not necessary to define all scales on every feature map; we just defined one scale per feature map. Here, in order to correspond five scales, we added P<sub>6</sub> on the basis of P<sub>5</sub> and it is the output of the max-pooling after the P<sub>5</sub>. Hence, according to this idea, we can generate all possible rectangular boxes (anchor [<xref ref-type="bibr" rid="CR9">9</xref>]) on the original image.</p><p id="Par35">In the RPN network, we filter N numbers of RoIs by a small convolution network. This small network determines the object possibility of each anchor, and we call this possibility anchor score. We sorted all the anchors by this score and take the top N high score boxes as RoI. Moreover, in order to adjust the position of each anchor, the small network also predicts the regression offsets of each anchor. Therefore, in FPN, there are several feature maps and this small network is shared with all feature maps, and the detail is shown in Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>.<fig id="Fig6"><label>Fig. 6</label><caption><p>The detail of the region proposal network. The left column is the output of the feature extracting. The middle content is the convolutional neural network. The right column is to classify and regress</p></caption><graphic xlink:href="41038_2018_137_Fig6_HTML" id="MO6"/></fig></p></sec><sec id="Sec13"><title>RPN training</title><p id="Par36">As shown in Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>, the outputs of the RPN network are score and regression offsets of each anchor. Here, we define two loss functions to train the RPN network. The first is the score loss <italic>L</italic><sub>rpnScore</sub>and the second is regression loss <italic>L</italic><sub>rpnReg</sub>.</p><p id="Par37">To calculate <italic>L</italic><sub>rpnScore</sub>, we assign two kinds of labels which are the positive label and the negative label to each anchor. The anchor which has an intersection over union (IOU) overlap higher than 0.7 with any ground-truth bounding box is a positive label, and the anchor which has an IOU overlap lower than 0.3 with all ground-truth boxes is a negative label. Here, in order to ensure that all the ground-truth boxes correspond to at least one anchor, we will label the highest IOU anchor with each ground-truth box as a positive label. Therefore, we can get all the positive and negative anchors. We encode these anchors into a sequence of 0 and 1, and the sequence is the objective output in the RPN target judgment. As it is shown in Fig.&#x000a0;<xref rid="Fig6" ref-type="fig">6</xref>, we apply the <italic>softmax</italic> function to the output of the RPN to get target possibility for all anchors. And then, we use the cross-entropy function to calculate the <italic>L</italic><sub>rpnScore</sub>.</p><p id="Par38">Then, we apply a linear function to the output of the RPN network and predict the regression parameters (<italic>t</italic><sup>&#x02217;</sup>). We calculate the regression offsets (<italic>t</italic>) of each positive anchor. The regression offsets are the same as [<xref ref-type="bibr" rid="CR8">8</xref>], and it contains four values (<italic>x</italic>, <italic>y</italic>, <italic>w</italic>, <italic>h</italic>). <italic>x</italic> and <italic>y</italic> are the offset ratios of the positive anchors&#x02019; center point based on the associated ground-truth boxes center point. Then, <italic>w</italic> and <italic>h</italic> are the logarithmic values of the aspect ratio of positive anchors and the associated ground-truth boxes. Finally, we used the <italic>smooth</italic><sub>L1</sub> to calculate the <italic>L</italic><sub><italic>rpnReg</italic></sub> which is shown in Eq.&#x000a0;<xref rid="Equ1" ref-type="">1</xref>. Here, we stipulate that just the positive anchor will contribute the <italic>L</italic><sub><italic>rpnReg</italic></sub>.<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="M1">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {L}_{rpnReg}\left({t}_i\right)=\frac{1}{N_{r\mathrm{eg}}}{\sum}_i{p}_i^{\ast }{L}_{r\mathrm{eg}}\left({t}_i,{t}_i^{\ast}\right) $$\end{document}</tex-math><mml:math id="M2" display="block"><mml:msub><mml:mi>L</mml:mi><mml:mtext mathvariant="italic">rpnReg</mml:mtext></mml:msub><mml:mfenced close=")" open="("><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>eg</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:msubsup><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msubsup><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>eg</mml:mi></mml:mrow></mml:msub><mml:mfenced close=")" open="(" separators=","><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msubsup><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msubsup></mml:mfenced></mml:math><graphic xlink:href="41038_2018_137_Article_Equ1.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par39">Here, <italic>i</italic> is the index of an anchor in the mini-batch and <inline-formula id="IEq1"><alternatives><tex-math id="M3">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {p}_i^{\ast } $$\end{document}</tex-math><mml:math id="M4" display="inline"><mml:msubsup><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msubsup></mml:math><inline-graphic xlink:href="41038_2018_137_Article_IEq1.gif"/></alternatives></inline-formula> is 1 if the anchor is positive; otherwise, p<sub>i</sub><sup>*</sup> is 0. Here, <italic>t</italic><sub><italic>i</italic></sub> and <inline-formula id="IEq2"><alternatives><tex-math id="M5">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {t}_i^{\ast } $$\end{document}</tex-math><mml:math id="M6" display="inline"><mml:msubsup><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msubsup></mml:math><inline-graphic xlink:href="41038_2018_137_Article_IEq2.gif"/></alternatives></inline-formula> are the four vectors representing the regression offset, and <italic>t</italic><sub><italic>i</italic></sub> represents regression offset of a positive anchor based on the associated ground-truth box. And <inline-formula id="IEq3"><alternatives><tex-math id="M7">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {t}_i^{\ast } $$\end{document}</tex-math><mml:math id="M8" display="inline"><mml:msubsup><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msubsup></mml:math><inline-graphic xlink:href="41038_2018_137_Article_IEq3.gif"/></alternatives></inline-formula> represents the predicted regression offset. The regression loss function is shown in Eq.&#x000a0;<xref rid="Equ2" ref-type="">2</xref>. The <italic>smooth</italic><sub>L1</sub> is defined in Eq.&#x000a0;<xref rid="Equ3" ref-type="">3</xref>.<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="M9">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {L}_{r\mathrm{eg}}\left(t,{t}^{\ast}\right)={\sum}_{i\in x,\mathrm{y},\mathrm{w},\mathrm{h}}s{\mathrm{mooth}}_{L_1}\left(t-{t}_i^{\ast}\right) $$\end{document}</tex-math><mml:math id="M10" display="block"><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>eg</mml:mi></mml:mrow></mml:msub><mml:mfenced close=")" open="(" separators=","><mml:mi>t</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup></mml:mfenced><mml:mo>=</mml:mo><mml:msub><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#x02208;</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">y</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">w</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:msub><mml:mi>s</mml:mi><mml:msub><mml:mtext>mooth</mml:mtext><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub><mml:mfenced close=")" open="("><mml:mrow><mml:mi>t</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msubsup></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="41038_2018_137_Article_Equ2.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par40">
<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="M11">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ s{\mathrm{mooth}}_{L_1}(x)=\left\{\begin{array}{c}0.5\ {x}^2\kern2.55em \mathrm{if}\ \left|x\right|&#x0003c;1\\ {}\left|x\right|-0.5\kern2.25em \mathrm{otherwise}\end{array}\right. $$\end{document}</tex-math><mml:math id="M12" display="block"><mml:mi>s</mml:mi><mml:msub><mml:mtext>mooth</mml:mtext><mml:msub><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub><mml:mfenced close=")" open="("><mml:mi>x</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced close="" open="{"><mml:mtable><mml:mtr><mml:mtd><mml:mn>0.5</mml:mn><mml:mspace width="0.25em"/><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mspace width="0.25em"/><mml:mspace width="2.3em"/><mml:mtext>if</mml:mtext><mml:mspace width="0.25em"/><mml:mfenced close="|" open="|"><mml:mi>x</mml:mi></mml:mfenced><mml:mo>&#x0003c;</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mfenced close="|" open="|"><mml:mi>x</mml:mi></mml:mfenced><mml:mo>&#x02212;</mml:mo><mml:mn>0.5</mml:mn><mml:mspace width="2.25em"/><mml:mtext>otherwise</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:mfenced></mml:math><graphic xlink:href="41038_2018_137_Article_Equ3.gif" position="anchor"/></alternatives></disp-formula>
</p><p id="Par41">We used Eq.&#x000a0;<xref rid="Equ4" ref-type="">4</xref> to make a detailed explanation for regression offset.<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="M13">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\displaystyle \begin{array}{cc}\begin{array}{c}{t}_x=\frac{x-{x}_a}{w_a}\\ {}{t}_w=\log \left(\frac{w}{w_a}\right)\end{array}&#x00026; \begin{array}{c}{t}_y=\frac{y-{y}_a}{h_a}\\ {}{t}_h=\log \left(\frac{h}{h_a}\right)\end{array}\\ {}\begin{array}{c}{t}_x^{\ast }=\frac{x^{\ast }-{x}_a}{w_a}\\ {}{t}_w^{\ast }=\log \left(\frac{w^{\ast }}{w_a}\right)\end{array}&#x00026; \begin{array}{c}{t}_y^{\ast }=\frac{y^{\ast }-{y}_a}{h_a}\\ {}{t}_h^{\ast }=\log \left(\frac{h^{\ast }}{h_a}\right)\end{array}\end{array}} $$\end{document}</tex-math><mml:math id="M14" display="block"><mml:mtable displaystyle="true"><mml:mtr><mml:mtd><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>t</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>t</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>log</mml:mo><mml:mfenced close=")" open="("><mml:mfrac><mml:mi>w</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mfrac></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:mtd><mml:mtd><mml:mtable><mml:mtr><mml:mtd><mml:msub><mml:mi>t</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>y</mml:mi><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>log</mml:mo><mml:mfenced close=")" open="("><mml:mfrac><mml:mi>h</mml:mi><mml:msub><mml:mi>h</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mfrac></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtable><mml:mtr><mml:mtd><mml:msubsup><mml:mi>t</mml:mi><mml:mi>x</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>t</mml:mi><mml:mi>w</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mo>log</mml:mo><mml:mfenced close=")" open="("><mml:mfrac><mml:msup><mml:mi>w</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:msub><mml:mi>w</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mfrac></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:mtd><mml:mtd><mml:mtable><mml:mtr><mml:mtd><mml:msubsup><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mo>log</mml:mo><mml:mfenced close=")" open="("><mml:mfrac><mml:msup><mml:mi>h</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msup><mml:msub><mml:mi>h</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mfrac></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math><graphic xlink:href="41038_2018_137_Article_Equ4.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par42">After choosing the RoIs from the anchors, we map the RoIs on the feature map for subsequent operation of the framework. But in our framework, we have four feature maps. Unlike generating anchors, we do not make each RoI correspond to a feature map. Considering that P<sub>2</sub> contains all image features, we map all RoIs to P<sub>2</sub>. After mapping, the three parallel branches handle the mapping results.</p></sec><sec id="Sec14"><title>Loss function</title><p id="Par43">In our framework, our loss contains five aspects. The RPN network contains two losses. In the parallel branches, there are three losses. We define the three losses as <italic>L</italic><sub>mCls</sub>, <italic>L</italic><sub>mBReg</sub>, and&#x000a0;<italic>L</italic><sub>mMask</sub>. Therefore, our final loss is L =<italic>L</italic><sub>rpnScore</sub>+<italic>L</italic><sub>rpnReg</sub>+<italic>L</italic><sub>mCls</sub>+<italic>L</italic><sub>mBReg</sub>+ <italic>L</italic><sub>mMask</sub>.</p><sec id="Sec15"><title>Class loss</title><p id="Par44">In Mask R-CNN, the author applied <italic>softmax</italic> on the output of the fully connected layer and used the cross-entropy function to calculate the class loss. This method was applied to solve the multi-class classification tasks. But in our task, our goal is simply to segment the burn wounds. Therefore, we used two classifiers to replace the multiple classifiers. We applied the <italic>sigmoid</italic> function on the output and used the cross-entropy function to calculate the loss. We used <italic>y</italic> to define the ground-truth of the N RoIs. The output of <italic>sigmoid</italic> is <italic>y</italic><sup>*</sup>. Then, the <italic>L</italic><sub>mCls</sub>&#x000a0;is Eq.&#x000a0;<xref rid="Equ5" ref-type="">5</xref>.<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="M15">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {L}_{\mathrm{mCls}}=\frac{1}{N}{\sum}_i^N\left({y}_i\ast \left(-\mathit{\log}{y}_i^{\ast}\right)+\left(1-{y}_i\right)\ast \left(-\log \left(1-{y}_i^{\ast}\right)\right)\right) $$\end{document}</tex-math><mml:math id="M16" display="block"><mml:msub><mml:mi>L</mml:mi><mml:mtext>mCls</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mi>i</mml:mi><mml:mi>N</mml:mi></mml:msubsup><mml:mfenced close=")" open="("><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&#x02217;</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mo mathvariant="italic">log</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msubsup></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>&#x02217;</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mo>&#x02212;</mml:mo><mml:mo>log</mml:mo><mml:mfenced close=")" open="("><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#x02212;</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>i</mml:mi><mml:mo>&#x02217;</mml:mo></mml:msubsup></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="41038_2018_137_Article_Equ5.gif" position="anchor"/></alternatives></disp-formula></p></sec><sec id="Sec16"><title>Bounding-box loss</title><p id="Par45">As mentioned above, the RPN network will predict the regression offset of each anchor. In the box branch, the input will be the coordinate of RoI. These coordinates are the result of the RoI that applied the regression offset of the RPN network. We then used the same way as <italic>L</italic><sub><italic>rpnReg</italic></sub> to calculate the <italic>L</italic><sub>mBReg.</sub></p></sec><sec id="Sec17"><title>Mask loss</title><p id="Par46">In Mask R-CNN, the author applied a small FCN [<xref ref-type="bibr" rid="CR14">14</xref>] network on the RoI. And in the mask branch, the author predicted the m&#x02009;&#x000d7;&#x02009;m mask. The mask is the output of the sigmoid function which is applied to each pixel. Then, the author calculated the mask loss according to the mask class to avoid the competition between classes and used the binary cross-entropy to define the loss.</p><p id="Par47">However, in this article, we just calculated the mask loss of the positive RoI and did not use the idea of competition between classes. Moreover, we defined the size of each ground-truth mask and predicted the mask as 28&#x02009;&#x000d7;&#x02009;28 to reduce memory consumption. Hence, the ground-truth RoI was scaled to 28&#x02009;&#x000d7;&#x02009;28 and padded with zero to avoid distortion. In the output of the mask branch, we will scale each RoI to the same size to calculate the mask loss.</p></sec><sec id="Sec18"><title>Regularization loss</title><p id="Par48">As mentioned above, we collected little data sets. And in order to prevent over-fitting of the model, we add the loss of regular term for entire loss function. We can see the details from the formula <xref rid="Equ6" ref-type="">6</xref></p><p id="Par49">
<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="M17">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {L}_{regLoss}=\lambda {\sum}_{i=1}^n\left({W}_i^2\cdotp \frac{1}{N_{w_i}}\right) $$\end{document}</tex-math><mml:math id="M18" display="block"><mml:msub><mml:mi>L</mml:mi><mml:mtext mathvariant="italic">regLoss</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mi>&#x003bb;</mml:mi><mml:msubsup><mml:mo>&#x02211;</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mfenced close=")" open="("><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>&#x000b7;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:mfrac></mml:mrow></mml:mfenced></mml:math><graphic xlink:href="41038_2018_137_Article_Equ6.gif" position="anchor"/></alternatives></disp-formula>
</p><p id="Par50">This is the L2 regularization loss which represents the weight decay and aims to reduce the weight values to fit data well. In the formula, <italic>W</italic><sub><italic>i</italic></sub> is the weight values of the <italic>i</italic>-th layer and <inline-formula id="IEq4"><alternatives><tex-math id="M19">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {N}_{w_i} $$\end{document}</tex-math><mml:math id="M20" display="inline"><mml:msub><mml:mi>N</mml:mi><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:msub></mml:math><inline-graphic xlink:href="41038_2018_137_Article_IEq4.gif"/></alternatives></inline-formula> is the size of the <italic>W</italic><sub><italic>i</italic></sub>. The <italic>&#x003bb;</italic> is a hyper-parameter which is set as 0.0001 here.</p></sec></sec><sec id="Sec19"><title>Training detail</title><p id="Par51">In order to obtain better training results, we did not randomly initialize weight in this framework. The initialization of the weight includes two parts. In the convolutional neural network, we used the pre-trained COCO model to initialize our backbone network. In the network head, we used the Gaussian distribution to initialize the weight values. Similar to the transfer learning, we fine-tune the convolutional neural network of our framework by collecting data.</p><p id="Par52">Moreover, we tried several convolutional networks to extract the feature map from the original image. These backbone networks are Residual Network-101 with Atrous Convolution (R101A), Residual Network-101 with Atrous Convolution in Feature Pyramid Network (R101FA), and InceptionV2-Residual Network with Atrous Convolution (IV2RA). Through the experiment, we find the R101FA backbone has the best segmentation result. Before training, the images are resized to a 1024 width for a proper input in the network. Then, similar to the [<xref ref-type="bibr" rid="CR10">10</xref>], the input data will undergo five convolution layers C<sub>1</sub>, C<sub>2</sub>, C<sub>3</sub>, C<sub>4</sub>, and C<sub>5</sub> which have strides of 2, 4, 8, 16, and 32 related to the input image.</p><p id="Par53">After extracting a feature map, the RPN network handled the output of the backbone network. First, the RPN network generated N anchors (the anchor scale is for the original image) on the center of the sliding window. Then, we calculated the IOU value (per anchor) to judge if the anchor is positive or negative. As in [<xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR9">9</xref>], each image has N-sampled RoIs which have a ratio of 1:3 of positive to negatives. Then, we pooled each positive anchor to a fixed size. After that, we connected a fully connected network to extract a 2048 dimensions feature vector. This vector is used for the classifier and the box regressor. At the same time, the RoIs will undergo two convolution layers, then we predicted the image mask.</p></sec><sec id="Sec20"><title>Burn area calculation</title><p id="Par54">Burn area calculation is an important part of burn diagnosis. The framework as mentioned above is an auxiliary technique for calculating burn area and has great significance for fast, convenient, and accurate burn area calculation. As it is shown in Fig.&#x000a0;<xref rid="Fig1" ref-type="fig">1</xref>, for example, the second method needs to manually mark the edge of the burn wound when calculating. Similar to the 3D application, it is not conducive for rapid treatment of patients. However, if we combine our segmentation framework with this software, we can get a more efficient and convenient area calculation tool. In a sense, we can apply our framework to the calculation of burn wound area.</p><p id="Par55">In our plan, we intend to combine the 3D modeling and mesh parameterization technology with our segmented framework which calculates the burn wound area. The calculation mainly consists of three steps:<list list-type="bullet"><list-item><p id="Par56">Step-1: Building a 3D model of patient pictures through 3D reconstruction technology.</p></list-item><list-item><p id="Par57">Step-2: Mapping the 3D model to the planar domain by mesh parameterization algorithm.</p></list-item><list-item><p id="Par58">Step-3: Segmenting the burn regions by using our framework and calculate the total body surface area (TBSA) value.</p></list-item></list></p><p id="Par59">Some 3D reconstruction technologies are already very mature, such as BodyTalk reconstruction system [<xref ref-type="bibr" rid="CR15">15</xref>] and Kinect reconstruction system [<xref ref-type="bibr" rid="CR16">16</xref>], which makes the 3D model building process easier. The mesh parameterization algorithm such as RiccFlow [<xref ref-type="bibr" rid="CR17">17</xref>] and Authalic [<xref ref-type="bibr" rid="CR18">18</xref>] make the second step easier to implement. Hence, our segmentation framework can achieve a faster, easier, and more accurate area calculation.</p></sec></sec><sec id="Sec21"><title>Results</title><sec id="Sec22"><title>Burn characterization</title><p id="Par60">There are four main depths of burn wounds: (i) superficial dermal burn, (ii) superficial partial thickness burn, (iii) deep partial thickness burn [<xref ref-type="bibr" rid="CR19">19</xref>], and (iv) full-thickness burn. Figure&#x000a0;<xref rid="Fig7" ref-type="fig">7</xref> shows the four depths of burns across four images.<fig id="Fig7"><label>Fig. 7</label><caption><p><bold>a</bold>&#x02013;<bold>d</bold> Different burn depth. From left to right is superficial, superficial thickness, deep partial thickness, and full-thickness burn</p></caption><graphic xlink:href="41038_2018_137_Fig7_HTML" id="MO7"/></fig></p><p id="Par61">In the past, image processing techniques for total or partial segmentation often use evolving curvilinear boundaries because of their adaptive capacity and modeling of the internal structures of the images [<xref ref-type="bibr" rid="CR19">19</xref>, <xref ref-type="bibr" rid="CR20">20</xref>]. In this article, we come to the conclusion that the burn does not exhibit uniform boundaries. Moreover, various depths of wounds make the segmentation work harder. The traditional technologies no longer work in the burn segmentation if we want to segment all burn situations. Hence, we adopt the deep learning framework to realize the segmentation.</p></sec><sec id="Sec23"><title>Segmentation results</title><p id="Par62">In this paper, our framework mainly segments the burn wounds without classifying the depth of the wound. However, in order to show the stability and generalization ability of our work, we selected 150 images to evaluate our method. With the help of professional doctors, we combined these images from the different burn area size images and different burn depth images.</p></sec><sec id="Sec24"><title>Segment different sizes</title><p id="Par63">The first advantage is that our model can segment the different sizes of the burn wounds. Through the experiment, our model expressed high robustness of the different burn wound sizes. We chose the four depths of burn wounds. As it is shown in Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref>, our model performed fine segmentation in the %TBSA &#x0003c;&#x02009;5% burn wound. Moreover, for the large burn area, our model also performed very well. This is also shown in Fig.&#x000a0;<xref rid="Fig8" ref-type="fig">8</xref>.<fig id="Fig8"><label>Fig. 8</label><caption><p>Segmentation results of different burn wounds sizes. The first and second rows show the %TBSA<italic>&#x0003c;</italic>&#x02009;5% wounds and the segmentation results of R101FA. The third and fourth rows show the %TBSA<italic>&#x0003e;</italic>&#x02009;20% wounds and the segmentation results of R101FA (<italic>R101FA</italic> residual network-101 with atrous convolution in feature pyramid network<italic>. TBSA</italic> total body surface area)</p></caption><graphic xlink:href="41038_2018_137_Fig8_HTML" id="MO8"/></fig></p></sec><sec id="Sec25"><title>Segment different depth</title><p id="Par64">There are many reasons to cause a burn such as hydrothermal fluid, high-temperature gas, and flame. In addition, these reasons can lead to different burn wounds depths. Because of the variance of each depth of burn, it increases the difficulty of segmentation. However, in our model, we can segment the different burn wound depths successfully. Figure&#x000a0;<xref rid="Fig9" ref-type="fig">9</xref> shows the segmentation results of different depth burns.<fig id="Fig9"><label>Fig. 9</label><caption><p>Segmentation results of different burn wound depths. These four lines are the superficial, superficial thickness, deep partial thickness, and full-thickness burn from top to bottom</p></caption><graphic xlink:href="41038_2018_137_Fig9_HTML" id="MO9"/></fig></p></sec><sec id="Sec26"><title>Method comparison</title><p id="Par65">We compared our method with traditional methods and modern methods.</p><sec id="Sec27"><title>Traditional methods</title><p id="Par66">In the image segmentation, the traditional methods always use the edge features or spectral feature of the image to accomplish the segmentation. In this article, we tried the Watershed Algorithm [<xref ref-type="bibr" rid="CR21">21</xref>] for our burn images. The Watershed Algorithm is based on the edge of the image and also labels the spectral feature on the image. Most of the time, this algorithm uses the color histogram to decide which color is to be the watershed. But in the burn images, there are many colors similar to the burn wound. This element made it difficult for the watershed algorithm to segment the burn wounds. We finally tried a different parameter to get better segmentation. As shown in Fig.&#x000a0;<xref rid="Fig10" ref-type="fig">10</xref>, we can see that the Watershed Algorithm could not show good segmentation in a complex picture environment.<fig id="Fig10"><label>Fig. 10</label><caption><p>Traditional methods compared. The first line is the result of the watershed algorithm and the second line is the result of R101FA (<italic>R101FA</italic> residual network-101 with atrous convolution in feature pyramid network)</p></caption><graphic xlink:href="41038_2018_137_Fig10_HTML" id="MO10"/></fig></p></sec><sec id="Sec28"><title>Modern methods</title><p id="Par67">In recent years, the image segmentation method which is based on deep learning achieves excellent performance. In this article, therefore, we chose different architectures to be the backbone network in our framework. These backbone networks are IV2RA [<xref ref-type="bibr" rid="CR22">22</xref>], R101A [<xref ref-type="bibr" rid="CR10">10</xref>], and R101FA.</p><p id="Par68">In training, for the best training effect, we set 20 epochs for IV2RA and R101A. But in our method, we set 16 epochs. Each epoch contained 1000 iterations. First, we will show the loss reduction of the different backbone networks. As shown in Fig.&#x000a0;<xref rid="Fig11" ref-type="fig">11</xref>, we see that our method can achieve a better loss reduction than two of the other backbone networks. Moreover, our method used fewer epochs than the other backbone networks.<fig id="Fig11"><label>Fig. 11</label><caption><p>Loss results of different backbone networks. <italic>IV2RA</italic> inceptionV2-residual network with atrous convolution, <italic>R101A</italic> residual network-101 with atrous convolution, and <italic>R101FA</italic> residual network-101 with atrous convolution in feature pyramid network</p></caption><graphic xlink:href="41038_2018_137_Fig11_HTML" id="MO11"/></fig></p><p id="Par69">During the evaluation, we chose 150 burn images to evaluate the different backbone networks. Here, because we only considered the wound segmentation effect, there was no need to use the mean average precision to evaluate the model. Hence, we chose the Dice coefficient (DC) [<xref ref-type="bibr" rid="CR23">23</xref>] to evaluate the percentage of segmentation accuracy from the ground truth. The DC measures the concordance between two enclosed areas. The formula is as follows:<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="M21">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \mathrm{DC}\%=100\frac{2 TP}{FP+2 TP+ FN} $$\end{document}</tex-math><mml:math id="M22" display="block"><mml:mi>DC</mml:mi><mml:mo>%</mml:mo><mml:mo>=</mml:mo><mml:mn>100</mml:mn><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant="italic">TP</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="italic">FP</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi mathvariant="italic">TP</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="italic">FN</mml:mi></mml:mrow></mml:mfrac></mml:math><graphic xlink:href="41038_2018_137_Article_Equ7.gif" position="anchor"/></alternatives></disp-formula></p><p id="Par70">In detail, the number of false positives is the FP value. The false positive represents the incorrectly segmented pixels. The FN is the number of false negatives. The false negative represents the target pixels that are not segmented. TP is the true positive. The true positive represents the correct segmentation pixels. Therefore, we calculate the DC value of the different backbone networks. The result is shown in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>.<table-wrap id="Tab1"><label>Table 1</label><caption><p>Average Dice&#x02019;s coefficient (DC) value and prediction speed per picture</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Model name</th><th>Average DC value</th><th>Prediction speed (per/second)</th></tr></thead><tbody><tr><td>R101FA (our method)</td><td>84.51<sup>*</sup></td><td>0.374<sup>+</sup></td></tr><tr><td>IV2RA</td><td>83.02</td><td>0.538</td></tr><tr><td>R101A</td><td>82.04</td><td>0.519</td></tr></tbody></table><table-wrap-foot><p>*The highest average DC value in different models</p><p><sup>+</sup>The fastest prediction speed in different models</p><p><italic>R101FA</italic> residual network-101 with atrous convolution in feature pyramid network, <italic>IV2RA</italic> inceptionV2-residual network with atrous convolution, <italic>R101A</italic> residual network-101 with atrous convolution</p></table-wrap-foot></table-wrap></p><p id="Par71">From the DC values, we discover that our backbone network R101FA has the highest accuracy. In other words, our model has a better result in burn image segmentation.</p><p id="Par72">To more fully evaluate our model, we chose the different burn images. We chose a total of 120 pictures of different burn depths. There were 20 superficial burns, 50 superficial thickness burns, 40 deep partial thickness burns, and 10 full-thickness burn. Because of a lack of burn images, for the full-thickness burn, we were only able to analyze 10. Then, we calculated the DC values of the different backbone networks.</p><p id="Par73">As shown in Table&#x000a0;<xref rid="Tab2" ref-type="table">2</xref>, our model showed better segmentation in the superficial, superficial thickness, and deep partial thickness. Perhaps due to the lack of full-thickness burn images, the results for full-thickness burn in our model were slightly worse than the other models.<table-wrap id="Tab2"><label>Table 2</label><caption><p>Dice&#x02019;s coefficient (DC) values of different burn depth in different models</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Burn depths</th><th colspan="3">Model name</th></tr><tr><th>R101FA (our method)</th><th>IV2RA</th><th>R101A</th></tr></thead><tbody><tr><td>Superficial</td><td>89.7<sup>*</sup></td><td>83.61</td><td>77.37</td></tr><tr><td>Superficial thickness</td><td>85.21<sup>*</sup></td><td>82.52</td><td>84.91</td></tr><tr><td>Deep partial thickness</td><td>84.54<sup>*</sup></td><td>84.44</td><td>81.96</td></tr><tr><td>Full-thickness burn</td><td>81.12</td><td>74.56</td><td>83.5<sup>*</sup></td></tr></tbody></table><table-wrap-foot><p>*The highest average DC value of this burn depth in different models</p><p><italic>R101FA</italic> residual network-101 with atrous convolution in feature pyramid network, <italic>IV2RA</italic> inceptionV2-residual network with atrous convolution, <italic>R101A</italic> residual network-101 with atrous convolution</p></table-wrap-foot></table-wrap></p><p id="Par74">On the other hand, there were many patients with extensive burns in the clinical. Therefore, our method needed to demonstrate excellent results in this aspect. Therefore, we selected different burn area size images to evaluate different models. The result is shown in Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>.<table-wrap id="Tab3"><label>Table 3</label><caption><p>Dice&#x02019;s coefficient (DC) values of different burn sizes in different models</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2">Burn sizes</th><th colspan="3">Model name</th></tr><tr><th>R101FA (our method)</th><th>IV2RA</th><th>R101A</th></tr></thead><tbody><tr><td>%TBSA <italic>&#x0003c;</italic>&#x02009;5%</td><td>83.6</td><td>86.48</td><td>86.63<sup>*</sup></td></tr><tr><td>5%&#x000a0;<italic>&#x0003c;</italic>&#x000a0;%TBSA<italic>&#x0003c;</italic>&#x02009;20%</td><td>86.13</td><td>88.06<sup>*</sup></td><td>84.85</td></tr><tr><td>%TBSA<italic>&#x0003e;</italic>&#x02009;20%</td><td>81.27<sup>*</sup></td><td>72.33</td><td>74.13</td></tr></tbody></table><table-wrap-foot><p>*The highest average DC value of this burn size in different models</p><p><italic>R101FA</italic> residual network-101 with atrous convolution in feature pyramid network, <italic>IV2RA</italic> inceptionV2-residual network with atrous convolution, <italic>R101A</italic> residual network-101 with atrous convolution</p></table-wrap-foot></table-wrap></p><p id="Par75">As shown in Table&#x000a0;<xref rid="Tab3" ref-type="table">3</xref>, the IV2RA has the highest average DC value for 5%&#x0003c; %TBSA &#x0003c;&#x02009;20% wounds and the R101A has the highest average DC value for the %TBSA &#x0003c;&#x02009;5% wounds. At the same time, our method has the highest average DC value for the %TBSA &#x0003e;&#x02009;20% wounds and also has good results in two other sizes.</p><p id="Par76">In order to ensure efficient hospital treatment, the prediction time for each picture must be short. We compared the prediction time of the different backbone networks. As shown in Table&#x000a0;<xref rid="Tab1" ref-type="table">1</xref>, our models needed only 0.37&#x02009;s to predict an image which was the fastest prediction speed.</p></sec></sec></sec><sec id="Sec29"><title>Discussion</title><p id="Par77">Burn image segmentation is the first step in the intelligent diagnosis of burn wounds. The precise segmentation is important to the follow-up treatment. In this article, we propose a state-of-the-art segmentation framework to segment the burn images. Compared with the traditional method, this method greatly improves the accuracy of segmentation and contributes immensely to the burn clinic. However, there are still some problems to be solved in this framework. As is known to all, deep learning technology requires a large number of data to ensure the accuracy of the model. In this framework, due to the complexity of data collection and annotation, we only provided almost 1000 pictures to train this model. This makes the model show bad segmentation results in some burn images. In addition, our framework cannot classify the depth of burn wound. In general, the evaluation of wound depth information needs to be combined with the professional knowledge of the doctor, which makes the process of data annotation extremely complicated and difficult for non-professionals to complete. Later, we will collect enough data sets to train the framework to improve the accuracy of the segmentation. Moreover, we will mark the burn wound depth information with the help of the professional, so as to classify the burn wound depth in this framework. And then we will apply our framework to calculate burn area.</p></sec><sec id="Sec30"><title>Conclusions</title><p id="Par78">This article proposed a new segmentation framework to segment the burn images based on deep learning technology. In the comparison experiment, we compared the feature extraction capability of different backbone networks. We found that the R101FA backbone network has the best result in accuracy and prediction speed. Finally, we achieved an average of 84.51% accuracy on 150 images. In practice, our method is more convenient than traditional methods and requires only a suitable RGB wound picture. It brings great benefits to the clinical treatment of the hospital.</p></sec></body><back><glossary><title>Abbreviations</title><def-list><def-item><term>COCO</term><def><p id="Par5">Common objects in context</p></def></def-item><def-item><term>DC</term><def><p id="Par6">Dice&#x02019;s coefficient</p></def></def-item><def-item><term>FCN</term><def><p id="Par7">Fully convolutional network</p></def></def-item><def-item><term>FPN</term><def><p id="Par8">Feature pyramid network</p></def></def-item><def-item><term>IOU</term><def><p id="Par9">Intersection over union</p></def></def-item><def-item><term>IV2RA</term><def><p id="Par10">InceptionV2-Residual Network with Atrous Convolution</p></def></def-item><def-item><term>R101A</term><def><p id="Par11">Residual Network-101 with Atrous Convolution</p></def></def-item><def-item><term>R101FA</term><def><p id="Par12">Residual Network-101 with Atrous Convolution in Feature Pyramid Network</p></def></def-item><def-item><term>R-CNN</term><def><p id="Par13">Regions with Convolutional Neural Network Features</p></def></def-item><def-item><term>ResNet</term><def><p id="Par14">Residual network</p></def></def-item><def-item><term>RoI</term><def><p id="Par15">Regions of interest</p></def></def-item><def-item><term>RPN</term><def><p id="Par16">Region proposal network</p></def></def-item><def-item><term>TBSA</term><def><p id="Par17">Total body surface area</p></def></def-item></def-list></glossary><ack><title>Acknowledgements</title><p>We thank the editor and all the anonymous reviewers for their valuable advice and suggestions to improve the quality of the current work. We also thank the data collectors for their data collection work. At the same time, we are grateful to the patients who provide their wound to be the data.</p><sec id="FPar1"><title>Funding</title><p id="Par79">This work was supported by the National Natural Science Foundation of China (Grant No.61772379) and the National Technological Action of Major Disease Prevention and Control (2018-ZX-01S-001).</p></sec><sec id="FPar2" sec-type="data-availability"><title>Availability of data and materials</title><p id="Par80">The data used in this study cannot be shared in compliance with Wuhan 607 Hospital NO.3 ethics and confidentiality.</p></sec></ack><notes notes-type="author-contribution"><title>Authors&#x02019; contributions</title><p>CJ and KS designed the framework and wrote the manuscript of this framework. CJ was the creator of this framework and conducted the experiment of the burn images. WX guided the screening of burn images, the classification of burn depth, and the assessment of the wound area. ZY played a major role in the collection of the data. All authors read and approved the final manuscript.</p></notes><notes notes-type="COI-statement"><sec id="FPar3"><title>Ethics approval and consent to participate</title><p>This study was approved by the Ethics Committee of the Wuhan Hospital NO.3 &#x00026; Tongren Hospital of Wuhan University and the ethics number is &#x0201c;KY2018&#x02013;058&#x0201d;. The patients used in this research have already signed an informed consent form.</p></sec><sec id="FPar4"><title>Consent for publication</title><p>Written informed consent was obtained from the patients for publication of this manuscript and any accompanying images. A copy of the written consent is available for review by the Editor-in-Chief of this journal.</p></sec><sec id="FPar5"><title>Competing interests</title><p>The authors declare that they have no competing interests.</p></sec></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gethin</surname><given-names>G</given-names></name><name><surname>Cowman</surname><given-names>S</given-names></name></person-group><article-title>Wound measurement comparing the use of acetate tracings and Visitrak digital planimetry</article-title><source>J Clin Nurs</source><year>2006</year><volume>15</volume><issue>4</issue><fpage>422</fpage><lpage>427</lpage><pub-id pub-id-type="doi">10.1111/j.1365-2702.2006.01364.x</pub-id><pub-id pub-id-type="pmid">16553755</pub-id></element-citation></ref><ref id="CR2"><label>2.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haghpanah</surname><given-names>S</given-names></name><name><surname>Bogie</surname><given-names>K</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Banks</surname><given-names>PG</given-names></name><name><surname>Ho</surname><given-names>CH</given-names></name></person-group><article-title>Reliability of electronic versus manual wound measurement techniques</article-title><source>Arch Phys Med Rehabil</source><year>2006</year><volume>87</volume><issue>10</issue><fpage>1396</fpage><lpage>1402</lpage><pub-id pub-id-type="doi">10.1016/j.apmr.2006.06.014</pub-id><pub-id pub-id-type="pmid">17023252</pub-id></element-citation></ref><ref id="CR3"><label>3.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rogers</surname><given-names>LC</given-names></name><name><surname>Bevilacqua</surname><given-names>NJ</given-names></name><name><surname>Armstrong</surname><given-names>DG</given-names></name><name><surname>Andros</surname><given-names>G</given-names></name></person-group><article-title>Digital planimetry results in more accurate wound measurements: a comparison to standard ruler measurements</article-title><source>J Diabetes Sci Technol</source><year>2010</year><volume>4</volume><issue>4</issue><fpage>799</fpage><lpage>802</lpage><pub-id pub-id-type="doi">10.1177/193229681000400405</pub-id><pub-id pub-id-type="pmid">20663440</pub-id></element-citation></ref><ref id="CR4"><label>4.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sheng</surname><given-names>WB</given-names></name><name><surname>Zeng</surname><given-names>D</given-names></name><name><surname>Wan</surname><given-names>Y</given-names></name><name><surname>Yao</surname><given-names>L</given-names></name><name><surname>Tang</surname><given-names>HT</given-names></name><name><surname>Xia</surname><given-names>ZF</given-names></name></person-group><article-title>BurnCalc assessment study of computer-aided individual three-dimensional burn area calculation</article-title><source>J Transl Med</source><year>2014</year><volume>12</volume><issue>1</issue><fpage>242</fpage><pub-id pub-id-type="doi">10.1186/s12967-014-0242-x</pub-id><pub-id pub-id-type="pmid">25204349</pub-id></element-citation></ref><ref id="CR5"><label>5.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheah</surname><given-names>AKW</given-names></name><name><surname>Kangkorn</surname><given-names>T</given-names></name><name><surname>Tan</surname><given-names>EH</given-names></name><name><surname>Loo</surname><given-names>ML</given-names></name><name><surname>Chong</surname><given-names>SJ</given-names></name></person-group><article-title>The validation study on a three-dimensional burn estimation smart-phone application: accurate, free and fast?</article-title><source>Burns Trauma</source><year>2018</year><volume>6</volume><issue>1</issue><fpage>7</fpage><pub-id pub-id-type="doi">10.1186/s41038-018-0109-0</pub-id><pub-id pub-id-type="pmid">29497619</pub-id></element-citation></ref><ref id="CR6"><label>6.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Girshick</surname><given-names>R</given-names></name><name><surname>Donahue</surname><given-names>J</given-names></name><name><surname>Darrell</surname><given-names>T</given-names></name><name><surname>Malik</surname><given-names>J</given-names></name></person-group><article-title>Rich feature hierarchies for accurate object detection and semantic segmentation</article-title><source>2014 IEEE conference on computer vision and pattern recognition</source><year>2014</year><fpage>580</fpage><lpage>587</lpage></element-citation></ref><ref id="CR7"><label>7.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uijlings</surname><given-names>JRR</given-names></name><name><surname>De Sande</surname><given-names>KEAV</given-names></name><name><surname>Gevers</surname><given-names>T</given-names></name><name><surname>Smeulders</surname><given-names>AWM</given-names></name></person-group><article-title>Selective search for object recognition</article-title><source>Int J Comput Vis</source><year>2013</year><volume>104</volume><issue>2</issue><fpage>154</fpage><lpage>171</lpage><pub-id pub-id-type="doi">10.1007/s11263-013-0620-5</pub-id></element-citation></ref><ref id="CR8"><label>8.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Girshick</surname><given-names>R</given-names></name></person-group><article-title>Fast R-CNN</article-title><source>The IEEE international conference on computer vision (ICCV)</source><year>2015</year><fpage>1440</fpage><lpage>1448</lpage></element-citation></ref><ref id="CR9"><label>9.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>He</surname><given-names>K</given-names></name><name><surname>Girshick</surname><given-names>RB</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group><article-title>Faster R-CNN: towards real-time object detection with region proposal networks</article-title><source>IEEE Trans Pattern Anal Mach Intell</source><year>2017</year><volume>39</volume><issue>6</issue><fpage>1137</fpage><lpage>1149</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2016.2577031</pub-id><pub-id pub-id-type="pmid">27295650</pub-id></element-citation></ref><ref id="CR10"><label>10.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group><article-title>Deep residual learning for image recognition</article-title><source>The IEEE conference on computer vision and pattern recognition (CVPR)</source><year>2016</year><fpage>770</fpage><lpage>778</lpage></element-citation></ref><ref id="CR11"><label>11.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>T</given-names></name><name><surname>Doll&#x000e1;r</surname><given-names>P</given-names></name><name><surname>Girshick</surname><given-names>R</given-names></name><name><surname>He</surname><given-names>K</given-names></name><name><surname>Hariharan</surname><given-names>B</given-names></name><name><surname>Belongie</surname><given-names>S</given-names></name></person-group><article-title>Feature pyramid networks for object detection</article-title><source>2017 IEEE conference on computer vision and pattern recognition (CVPR)</source><year>2017</year><fpage>936</fpage><lpage>944</lpage></element-citation></ref><ref id="CR12"><label>12.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>LC</given-names></name><name><surname>Papandreou</surname><given-names>G</given-names></name><name><surname>Kokkinos</surname><given-names>I</given-names></name><name><surname>Murphy</surname><given-names>K</given-names></name><name><surname>Yuille</surname><given-names>AL</given-names></name></person-group><article-title>DeepLab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</article-title><source>IEEE Trans Pattern Anal Mach Intell</source><year>2018</year><volume>40</volume><issue>4</issue><fpage>834</fpage><lpage>848</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2017.2699184</pub-id><pub-id pub-id-type="pmid">28463186</pub-id></element-citation></ref><ref id="CR13"><label>13.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Gkioxari</surname><given-names>G</given-names></name><name><surname>Dollar</surname><given-names>P</given-names></name><name><surname>Girshick</surname><given-names>R</given-names></name></person-group><article-title>Mask R-CNN</article-title><source>IEEE Trans Pattern Anal Mach Intell</source><year>2017</year><volume>PP</volume><issue>99</issue><fpage>1</fpage></element-citation></ref><ref id="CR14"><label>14.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Long</surname><given-names>J</given-names></name><name><surname>Shelhamer</surname><given-names>E</given-names></name><name><surname>Darrell</surname><given-names>T</given-names></name></person-group><article-title>Fully convolutional networks for semantic segmentation</article-title><source>Comput Vis pattern Recognit</source><year>2015</year><fpage>3431</fpage><lpage>3440</lpage></element-citation></ref><ref id="CR15"><label>15.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Streuber</surname><given-names>S</given-names></name><name><surname>Quiros-Ramirez</surname><given-names>MA</given-names></name><name><surname>Hill</surname><given-names>MQ</given-names></name><name><surname>Hahn</surname><given-names>CA</given-names></name><name><surname>Zuffi</surname><given-names>S</given-names></name><name><surname>O&#x02019;Toole</surname><given-names>A</given-names></name><etal/></person-group><article-title>Body talk: crowdshaping realistic 3D avatars with words</article-title><source>ACM Trans Graph</source><year>2016</year><volume>35</volume><issue>4</issue><fpage>54</fpage><pub-id pub-id-type="doi">10.1145/2897824.2925981</pub-id></element-citation></ref><ref id="CR16"><label>16.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>G</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>B</given-names></name><name><surname>Zeng</surname><given-names>J</given-names></name><name><surname>Lu</surname><given-names>G</given-names></name><name><surname>Zhang</surname><given-names>D</given-names></name></person-group><article-title>Reconstructing 3D human models with a Kinect</article-title><source>Comput Animat Virtual Worlds</source><year>2016</year><volume>27</volume><issue>1</issue><fpage>72</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1002/cav.1632</pub-id></element-citation></ref><ref id="CR17"><label>17.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>M</given-names></name><name><surname>Guo</surname><given-names>R</given-names></name><name><surname>Zeng</surname><given-names>W</given-names></name><name><surname>Luo</surname><given-names>F</given-names></name><name><surname>Yau</surname><given-names>ST</given-names></name><name><surname>Gu</surname><given-names>X</given-names></name></person-group><article-title>The unified discrete surface Ricci flow</article-title><source>Graph Model</source><year>2014</year><volume>76</volume><issue>5</issue><fpage>321</fpage><lpage>339</lpage><pub-id pub-id-type="doi">10.1016/j.gmod.2014.04.008</pub-id></element-citation></ref><ref id="CR18"><label>18.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zou</surname><given-names>G</given-names></name><name><surname>Hu</surname><given-names>J</given-names></name><name><surname>Gu</surname><given-names>X</given-names></name><name><surname>Hua</surname><given-names>J</given-names></name></person-group><article-title>Authalic parameterization of general surfaces using lie advection</article-title><source>IEEE Trans Vis Comput Graph</source><year>2011</year><volume>17</volume><issue>12</issue><fpage>2005</fpage><lpage>2014</lpage><pub-id pub-id-type="doi">10.1109/TVCG.2011.171</pub-id><pub-id pub-id-type="pmid">22034318</pub-id></element-citation></ref><ref id="CR19"><label>19.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pinero</surname><given-names>BA</given-names></name><name><surname>Serrano</surname><given-names>C</given-names></name><name><surname>Acha</surname><given-names>JI</given-names></name></person-group><article-title>Segmentation of burn images using the L*u*v* space and classification of their depths by color and texture imformation</article-title><source>Wirel Netw</source><year>2002</year><volume>6</volume><issue>1</issue><fpage>17</fpage><lpage>30</lpage></element-citation></ref><ref id="CR20"><label>20.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garcia-Zapirain</surname><given-names>B</given-names></name><name><surname>Shalaby</surname><given-names>A</given-names></name><name><surname>El-Baz</surname><given-names>A</given-names></name><name><surname>Elmaghraby</surname><given-names>A</given-names></name></person-group><article-title>Automated framework for accurate segmentation of pressure ulcer images</article-title><source>Comput Biol Med</source><year>2017</year><volume>90</volume><fpage>137</fpage><lpage>145</lpage><pub-id pub-id-type="doi">10.1016/j.compbiomed.2017.09.015</pub-id><pub-id pub-id-type="pmid">28987989</pub-id></element-citation></ref><ref id="CR21"><label>21.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vincent</surname><given-names>L</given-names></name><name><surname>Soille</surname><given-names>P</given-names></name></person-group><article-title>Watersheds in digital spaces: an efficient algorithm based on immersion simulations</article-title><source>IEEE Trans Pattern Anal Mach Intell</source><year>1991</year><volume>13</volume><issue>6</issue><fpage>583</fpage><lpage>598</lpage><pub-id pub-id-type="doi">10.1109/34.87344</pub-id></element-citation></ref><ref id="CR22"><label>22.</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>C</given-names></name><name><surname>Ioffe</surname><given-names>S</given-names></name><name><surname>Vanhoucke</surname><given-names>V</given-names></name><name><surname>Alemi</surname><given-names>A</given-names></name></person-group><article-title>Inception-v4, inception-ResNet and the impact of residual connections on learning</article-title><source>Natl Conf Artif Intell</source><year>2016</year><fpage>4278</fpage><lpage>4284</lpage></element-citation></ref><ref id="CR23"><label>23.</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zou</surname><given-names>KH</given-names></name><name><surname>Warfield</surname><given-names>SK</given-names></name><name><surname>Bharatha</surname><given-names>A</given-names></name><name><surname>Tempany</surname><given-names>CMC</given-names></name><name><surname>Kaus</surname><given-names>MR</given-names></name><name><surname>Haker</surname><given-names>SJ</given-names></name><etal/></person-group><article-title>Statistical validation of image segmentation quality based on a spatial overlap index: scientific reports</article-title><source>Acad Radiol</source><year>2004</year><volume>11</volume><issue>2</issue><fpage>178</fpage><pub-id pub-id-type="doi">10.1016/S1076-6332(03)00671-8</pub-id><pub-id pub-id-type="pmid">14974593</pub-id></element-citation></ref></ref-list></back></article>